===============================
SECTION 2.3

1. Different values for M with max_iter = 5 and eps = 0.5

Accuracy with M = 1 is 93.333333
Accuracy with M = 2 is 86.666667
Accuracy with M = 3 is 93.333333
Accuracy with M = 4 is 93.333333
Accuracy with M = 5 is 100.000000
Accuracy with M = 6 is 100.000000
Accuracy with M = 7 is 100.000000
Accuracy with M = 8 is 100.000000

The classification accuracy goes down as the number of components decreases.
However, the accuracy rate fluctuates as it depends alot on the random 
initialization of the gaussian means.

2. Different values for epsilon with M = 8 and max_iter = 5

Accuracy with epsilon = 0.5 is 100.000000
Accuracy with epsilon = 1 is 100.000000
Accuracy with epsilon = 100 is 100.000000
Accuracy with epsilon = 1000 is 100.000000
Accuracy with epsilon = 10000 is 93.333333
Accuracy with epsilon = 100000 is 93.333333
Accuracy with epsilon = 1000000 is 93.333333
Accuracy with epsilon = 10000000 is 93.333333

The classification accuracy goes down as epsilon increases.
However, the accuracy rate only goes down slightly even with very large eplison value.
This might due to the fact that the model is too simple and converge too quickly, 
and the decrease in accuracy is probably because not all models can fully converge with the given eplison.

3. Different values for the number of speakers with M = 8, eps = 0.5, and max_iter = 5

Accuracy with num_speakers = 1 is 6.666667
Accuracy with num_speakers = 5 is 33.333333
Accuracy with num_speakers = 10 is 66.666667
Accuracy with num_speakers = 15 is 100.000000
Accuracy with num_speakers = 20 is 100.000000
Accuracy with num_speakers = 25 is 100.000000
Accuracy with num_speakers = 30 is 100.000000

The classification accuracy goes down significantly as the number of speaker decreases.
This is because the less training data that we have, the less accuracy the Gaussian Mixture Models are,
because the models simply don't have enough information to match up with the testing data.

1) How might you improve the classification accuracy of the Gaussian mixtures,
without adding more training data?

We might imrpove the accuracy by relaxing on the assumption that the dimensions 
of the data are independent, and enables us to use the full covariance matrix 
for correlations instead of just using the diagonal. The full covariance matrix
will have a better correlation between feature components.

2) When would your classifier decide that a given test utterance comes from none
of the trained speaker models, how would your classifier come to this decision?

Set a minimum threshold for the variance of the likelihood, and if the test 
doesn't converge after several iterations, then we can conclude that the test utterance
doesn't belong to any of the trained speaker model. 

3) Can you think of some alternative models for doing speaker identification
that don't use Gaussion mixtures?

Speaker identification methods that don't use Gaussian mixtures are 
Vector Quantization, Support Vector Machines, Neural Networks 
using hidden layer(s) with non-linear sigmoid functions, and backward 
propagation to calculate correlations between the data.

===============================
SECTION 3.1

Phoneme: sil    # correct: 53    # total: 60    accuracy: 0.883 
Phoneme: n      # correct: 33    # total: 46    accuracy: 0.717 
Phoneme: aw     # correct: 0     # total: 4     accuracy: 0.000 
Phoneme: hv     # correct: 3     # total: 5     accuracy: 0.600 
Phoneme: ih     # correct: 8     # total: 34    accuracy: 0.235 
Phoneme: r      # correct: 9     # total: 34    accuracy: 0.264 
Phoneme: ix     # correct: 14    # total: 44    accuracy: 0.318 
Phoneme: z      # correct: 16    # total: 30    accuracy: 0.533 
Phoneme: tcl    # correct: 21    # total: 40    accuracy: 0.525 
Phoneme: t      # correct: 15    # total: 28    accuracy: 0.535 
Phoneme: uw     # correct: 1     # total: 10    accuracy: 0.100 
Phoneme: l      # correct: 31    # total: 46    accuracy: 0.673 
Phoneme: ey     # correct: 4     # total: 14    accuracy: 0.285 
Phoneme: m      # correct: 21    # total: 31    accuracy: 0.677 
Phoneme: aa     # correct: 5     # total: 13    accuracy: 0.384 
Phoneme: v      # correct: 7     # total: 15    accuracy: 0.466 
Phoneme: el     # correct: 0     # total: 9     accuracy: 0.000 
Phoneme: d      # correct: 7     # total: 29    accuracy: 0.241 
Phoneme: ao     # correct: 4     # total: 12    accuracy: 0.333 
Phoneme: q      # correct: 15    # total: 28    accuracy: 0.535 
Phoneme: eh     # correct: 7     # total: 30    accuracy: 0.233 
Phoneme: iy     # correct: 22    # total: 43    accuracy: 0.511 
Phoneme: dx     # correct: 1     # total: 9     accuracy: 0.111 
Phoneme: axr    # correct: 3     # total: 10    accuracy: 0.300 
Phoneme: ay     # correct: 7     # total: 15    accuracy: 0.466 
Phoneme: f      # correct: 11    # total: 15    accuracy: 0.733 
Phoneme: er     # correct: 3     # total: 6     accuracy: 0.500 
Phoneme: s      # correct: 39    # total: 43    accuracy: 0.906 
Phoneme: pau    # correct: 0     # total: 6     accuracy: 0.000 
Phoneme: dh     # correct: 4     # total: 22    accuracy: 0.181 
Phoneme: ch     # correct: 0     # total: 7     accuracy: 0.000 
Phoneme: dcl    # correct: 17    # total: 33    accuracy: 0.515 
Phoneme: jh     # correct: 1     # total: 6     accuracy: 0.166 
Phoneme: en     # correct: 0     # total: 5     accuracy: 0.000 
Phoneme: ax     # correct: 4     # total: 27    accuracy: 0.148 
Phoneme: pcl    # correct: 3     # total: 12    accuracy: 0.250 
Phoneme: p      # correct: 5     # total: 12    accuracy: 0.416 
Phoneme: epi    # correct: 2     # total: 16    accuracy: 0.125 
Phoneme: ah     # correct: 3     # total: 17    accuracy: 0.176 
Phoneme: w      # correct: 10    # total: 25    accuracy: 0.400 
Phoneme: uh     # correct: 0     # total: 5     accuracy: 0.000 
Phoneme: ow     # correct: 6     # total: 13    accuracy: 0.461 
Phoneme: hh     # correct: 0     # total: 3     accuracy: 0.000 
Phoneme: ae     # correct: 22    # total: 30    accuracy: 0.733 
Phoneme: kcl    # correct: 23    # total: 35    accuracy: 0.657 
Phoneme: k      # correct: 21    # total: 28    accuracy: 0.750 
Phoneme: oy     # correct: 0     # total: 6     accuracy: 0.000 
Phoneme: gcl    # correct: 3     # total: 10    accuracy: 0.300 
Phoneme: g      # correct: 1     # total: 8     accuracy: 0.125 
Phoneme: sh     # correct: 5     # total: 8     accuracy: 0.625 
Phoneme: bcl    # correct: 5     # total: 10    accuracy: 0.500 
Phoneme: b      # correct: 3     # total: 14    accuracy: 0.214 
Phoneme: ng     # correct: 0     # total: 6     accuracy: 0.000 
Phoneme: ux     # correct: 1     # total: 11    accuracy: 0.090 
Phoneme: y      # correct: 2     # total: 4     accuracy: 0.500 
Phoneme: th     # correct: 0     # total: 6     accuracy: 0.000 
Phoneme: nx     # correct: 0     # total: 7     accuracy: 0.000 
Phoneme: em     # correct: 0     # total: 1     accuracy: 0.000 

M = Number of mixture
Q = Number of hidden state
P = Proportion of data(%)
D = Dimensionaility

Accuracy for the default case (M = 8; Q = 3; P = 100%; D = 14) is 0.457117 

==========================================================================
SECTION 3.2

M = Number of mixture
Q = Number of hidden state
P = Proportion of data(%)
D = Dimensionaility

M       Q                           P      D       Accuracy
_____________________________________________________________

8       3                        30 %      14      0.222628 
8       3                        60 %      14      0.387774 
8       5                        30 %      14      0.192518 
8       5                        60 %      14      0.354927 

8       3                        30 %      7       0.294708 
8       3                        60 %      7       0.355839 
8       5                        30 %      7       0.267336 
8       5                        60 %      7       0.364051 

4       3                        30 %      14      0.291058 
4       3                        60 %      14      0.421533 
4       5                        30 %      14      0.263686 
4       5                        60 %      14      0.415146 

4       3                        30 %      7       0.312956 
4       3                        60 %      7       0.388686 
4       5                        30 %      7       0.292883 
4       5                        60 %      7       0.366788 

(M) Reducing the number of mixtures per state (M) improves the accuracy rate by about 3% - 8%.

Probably because the model starts to overfit the training data as the number of mixtures increase,
thus, lowering the accuracy rate. 

(Q) Increasing the number of states (Q) from 3 to 5 decreases the accuracy rate for about 1% - 3%.

This shows that 3 hidden states is the ideal number of state for the given model and training data available.
Any increase in hidden states result in overfit of the training data.

(P) The proportion of the training data for each phoneme is set to 30% and 60%.
Increasing the training data prportion has the greatest impact and increases 
on the accuracy rate for all cases, showing an increase of 6% - 16%.

This is mostly due to the fact that there are more data available for training, which significantly increases
the resemblement of the actual phoneme, thus, improving the accuracy rate as the training data proportion increases.

(D) In most of the cases lowering the dimension of data from 14 to 7 increases the accuracy rate.

This is probably because MFCC data's dimensions are highly uncorrelated, thus,
changing dimensions may or may not have an impact of the performance depending on the case. 


In conclusion, I believe we can improve the classification accuracy of model without adding more training data,
by finding the most optimal parameters for the model that doesn't overfit and underfit the training data.


==========================================================================
SECTION 3.3

Reference : Now here is truly a marvel.
Hypothesis: Now here is truly hey marvel.
SE: 0.166667
IE: 0.000000
DE: 0.000000
Total: 0.166667

Reference : The cartoon features a muskrat and a tadpole.
Hypothesis: Cat tune features a muskrat and a tadpole.
SE: 0.250000
IE: 0.000000
DE: 0.000000
Total: 0.250000

Reference : Just let me die in peace.
Hypothesis: Just let me die in peace.
SE: 0.000000
IE: 0.000000
DE: 0.000000
Total: 0.000000

Reference : The sculptor looked at him, bugeyed and amazed, angry.
Hypothesis: The sculptor looked at him, bug I'd and amazed, angry.
SE: 0.111111
IE: 0.111111
DE: 0.000000
Total: 0.222222

Reference : A flash illumined the trees as a crooked bolt twigged in several directions.
Hypothesis: A flash illuminated the trees as crook bolt tweaked several directions.
SE: 0.230769
IE: 0.000000
DE: 0.153846
Total: 0.384615

Reference : This is particularly true in site selection.
Hypothesis: This is particularly true sight selection.
SE: 0.142857
IE: 0.000000
DE: 0.142857
Total: 0.285714

Reference : We would lose our export markets and deny ourselves the imports we need.
Hypothesis: We would lose sour expert markets deny ourselves the imports we need.
SE: 0.153846
IE: 0.000000
DE: 0.076923
Total: 0.230769

Reference : Count the number of teaspoons of soysauce that you add.
Hypothesis: Compton number of teaspoons of so he sauce that you add.
SE: 0.200000
IE: 0.200000
DE: 0.100000
Total: 0.500000

Reference : Finally he asked, do you object to petting?
Hypothesis: Finally he asked, do you object to petting?
SE: 0.000000
IE: 0.000000
DE: 0.000000
Total: 0.000000

Reference : Draw every outer line first, then fill in the interior.
Hypothesis: Draw every other line first, then fill into interior.
SE: 0.200000
IE: 0.000000
DE: 0.100000
Total: 0.300000

Reference : Change involves the displacement of form.
Hypothesis: Change involves the displacement of fawn.
SE: 0.166667
IE: 0.000000
DE: 0.000000
Total: 0.166667

Reference : To his puzzlement, there suddenly was no haze.
Hypothesis: Two is puzzle mint, there suddenly was no haze.
SE: 0.375000
IE: 0.125000
DE: 0.000000
Total: 0.500000

Reference : Don't ask me to carry an oily rag like that.
Hypothesis: Donna's me to carry oily rag like that.
SE: 0.100000
IE: 0.000000
DE: 0.200000
Total: 0.300000

Reference : The full moon shone brightly that night.
Hypothesis: The the full moon shone brightly that night.
SE: 0.000000
IE: 0.142857
DE: 0.000000
Total: 0.142857

Reference : Tugboats are capable of hauling huge loads.
Hypothesis: Tugboats are capable falling huge loads.
SE: 0.142857
IE: 0.000000
DE: 0.142857
Total: 0.285714

Reference : Did dad do academic bidding?
Hypothesis: Did tatoo academic bidding?
SE: 0.200000
IE: 0.000000
DE: 0.200000
Total: 0.400000

Reference : She had your dark suit in greasy wash water all year.
Hypothesis: See add your dark suit and greasy wash water all year.
SE: 0.272727
IE: 0.000000
DE: 0.000000
Total: 0.272727

Reference : The thick elm forest was nearly overwhelmed by Dutch Elm Disease.
Hypothesis: The thick forest was nearly over helmed by Dutch Elm Disease.
SE: 0.090909
IE: 0.090909
DE: 0.090909
Total: 0.272727

Reference : Count the number of teaspoons of soysauce that you add.
Hypothesis: Cow ten number of teaspoons of soysauce that you add.
SE: 0.200000
IE: 0.000000
DE: 0.000000
Total: 0.200000

Reference : Norwegian sweaters are made of lamb's wool.
Hypothesis: Norwegian sweaters are made of lamb's wool.
SE: 0.000000
IE: 0.000000
DE: 0.000000
Total: 0.000000

Reference : We think differently.
Hypothesis: We think differently.
SE: 0.000000
IE: 0.000000
DE: 0.000000
Total: 0.000000

Reference : A toothpaste tube should be squeezed from the bottom.
Hypothesis: A too pays too should be squeezed from the bottom.
SE: 0.222222
IE: 0.111111
DE: 0.000000
Total: 0.333333

Reference : Ran away on a black night with a lawful wedded man.
Hypothesis: Ran away on a black night with an awful wedded man.
SE: 0.181818
IE: 0.000000
DE: 0.000000
Total: 0.181818

Reference : Don't ask me to carry an oily rag like that.
Hypothesis: Down ask me to carry an oily rag like that.
SE: 0.100000
IE: 0.000000
DE: 0.000000
Total: 0.100000

Reference : Don't ask me to carry an oily rag like that.
Hypothesis: Don't ask me to carry an oily rag like that.
SE: 0.000000
IE: 0.000000
DE: 0.000000
Total: 0.000000

Reference : Index words and electronic switches may be reserved in the following ways.
Hypothesis: Index words an electronic switches may be reserved in the following way.
SE: 0.166667
IE: 0.000000
DE: 0.000000
Total: 0.166667

Reference : The avalanche triggered a minor earthquake.
Hypothesis: The avalanche triggered minor earth way.
SE: 0.166667
IE: 0.166667
DE: 0.166667
Total: 0.500000

Reference : Don't ask me to carry an oily rag like that.
Hypothesis: Donna's me to carry an oily rag like that.
SE: 0.100000
IE: 0.000000
DE: 0.100000
Total: 0.200000

Reference : The thick elm forest was nearly overwhelmed by Dutch Elm Disease.
Hypothesis: The thick elm for his was nail he over well bye touch Elm Disease.
SE: 0.454545
IE: 0.272727
DE: 0.000000
Total: 0.727273

Reference : When all else fails, use force.
Hypothesis: When hall else fails, use forks.
SE: 0.333333
IE: 0.000000
DE: 0.000000
Total: 0.333333

Final SE: 0.165385
Final IE: 0.042308
Final DE: 0.050000

Final LEV DIST: 0.257692
